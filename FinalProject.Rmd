---
title: "R Notebook"
output: html_notebook
---

# ADEC 7430 Final Project

## Dmitriy Borzhkovskiy, Kayla Burt, and Ellison Taylor, due 10/18/2020

## Create working directy, load charity data set

```{r}
# load the data
rm(list=ls())
setwd("/Users/dimz/Desktop/Boston College/ADEC7430-Applied Machine Learning/")
charity <- read.csv("Final/charity.csv") # load the "charity.csv" file
attach(charity)
```

## Call appropriate Libraries

```{r}
RNGversion("3.5.3")
library(psych)
library(corrplot)
library(corrgram)
library(ggplot2)
library(purrr)
library(tidyr)
library(boot)
library(ISLR)
library(lars)
library(leaps)
library(glmnet)
library(pls)
```


## 1 - exploratory data analysis

```{r}
describe(charity)
summary(charity)
charity = charity[,-1]
colnames(charity)
str(charity)

#correlation

par(mfrow=c(1,1))
corrplot(corrgram(charity), method="square")

ctrain<- subset(charity,charity$part=="train")
ctest<- subset(charity,charity$part=="test")
cvalid<- subset(charity,charity$part=="valid")
```

##visually represent the numeric variables

```{r}


ctrain[,-1] %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot(aes(value)) + 
  facet_wrap(~ key, scales = "free") + 
  geom_density()
```

## Visually represent the categorical variables

```{r}

charity.cat <- c("reg1","reg2","reg3","reg4","home","chld", "hinc", "genf","wrat", "donr","part")

ctrain[,charity.cat] %>%
  gather() %>%
  ggplot(aes(value)) + 
  facet_wrap(~key, scales = "free") + 
  geom_bar()
par(mfrow=c(5,5))
boxplot(ctrain$agif)
boxplot(ctrain$avhv)
boxplot(ctrain$chld)
boxplot(ctrain$damt)
boxplot(ctrain$donr)
boxplot(ctrain$genf)
boxplot(ctrain$hinc)
boxplot(ctrain$home)
boxplot(ctrain$inca)
boxplot(ctrain$incm)
boxplot(ctrain$lgif)
boxplot(ctrain$npro)
boxplot(ctrain$plow)
boxplot(ctrain$reg2)
boxplot(ctrain$reg3)
boxplot(ctrain$reg4)
boxplot(ctrain$tdon) 
boxplot(ctrain$tgif)
boxplot(ctrain$tlag)
boxplot(ctrain$wrat)

```
```{r}
##charity <- read.csv("~/Desktop/School work/BC Data Analytics/ADEC7430-KBFinal/charity.csv")
##View(charity)

```




### From Provided Code: Transformation of 'avhv', create training and test sets

```{r}
# predictor transformations

charity.t <- charity
charity.t$avhv <- log(charity.t$avhv)
# add further transformations if desired
# for example, some statistical methods can struggle when predictors are highly skewed

# set up data for analysis

data.train <- charity.t[charity$part=="train",]
x.train <- data.train[,2:21]
c.train <- data.train[,22] # donr
n.train.c <- length(c.train) # 3984
y.train <- data.train[c.train==1,23] # damt for observations with donr=1
n.train.y <- length(y.train) # 1995

data.valid <- charity.t[charity$part=="valid",]
x.valid <- data.valid[,2:21]
c.valid <- data.valid[,22] # donr
n.valid.c <- length(c.valid) # 2018
y.valid <- data.valid[c.valid==1,23] # damt for observations with donr=1
n.valid.y <- length(y.valid) # 999

data.test <- charity.t[charity$part=="test",]
n.test <- dim(data.test)[1] # 2007
x.test <- data.test[,2:21]

x.train.mean <- apply(x.train, 2, mean)
x.train.sd <- apply(x.train, 2, sd)
x.train.std <- t((t(x.train)-x.train.mean)/x.train.sd) # standardize to have zero mean and unit sd
apply(x.train.std, 2, mean) # check zero mean
apply(x.train.std, 2, sd) # check unit sd
data.train.std.c <- data.frame(x.train.std, donr=c.train) # to classify donr
data.train.std.y <- data.frame(x.train.std[c.train==1,], damt=y.train) # to predict damt when donr=1

x.valid.std <- t((t(x.valid)-x.train.mean)/x.train.sd) # standardize using training mean and sd
data.valid.std.c <- data.frame(x.valid.std, donr=c.valid) # to classify donr
data.valid.std.y <- data.frame(x.valid.std[c.valid==1,], damt=y.valid) # to predict damt when donr=1

x.test.std <- t((t(x.test)-x.train.mean)/x.train.sd) # standardize using training mean and sd
data.test.std <- data.frame(x.test.std)
```

## PREDICTION MODELING 

###From Provided Code: Least squares regression

```{r}
## Model 1
model.pred1 <- lm(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + wrat + 
                  avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif, 
                data.train.std.y)
summary(model.pred1)

pred.valid.pred1 <- predict(model.pred1, newdata = data.valid.std.y) 
mean((y.valid - pred.valid.pred1)^2) 

sd((y.valid - pred.valid.pred1)^2)/sqrt(n.valid.y) 


## Model 2
model.pred2 <- lm(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + 
                  avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif, 
                data.train.std.y)
summary(model.pred2)


pred.valid.pred2 <- predict(model.pred2, newdata = data.valid.std.y) 
mean((y.valid - pred.valid.pred2)^2)

sd((y.valid - pred.valid.pred2)^2)/sqrt(n.valid.y) 

yhat.test <- predict(model.pred2, newdata = data.test.std) 

```

## Our Prediction Models: OLS, Best subset selection with k-fold cv, Principal Components Regression, Partial Least Squares, Ridge Regression, Lasso Regression

### Third OLS Regression: Model three only includes variables whose coefficients had P-values below 0.05 in model two.

```{r}

model.pred3<- lm(damt ~ reg3 + reg4 + home + chld + hinc + genf + 
                 incm +  plow + npro + rgif + tdon + agif, 
                data.train.std.y)
summary(model.pred3)

pred.valid.pred3 <- predict(model.pred3, newdata = data.valid.std.y) # validation predictions
mpe.pred3<-mean((y.valid - pred.valid.pred3)^2) 
mpe.pred3
std.err.pred3<-sd((y.valid - pred.valid.pred2)^2)/sqrt(n.valid.y) 
std.err.pred3
## The mpe is very slightly higher for model three than model two.
```



```{r}
## Prediction model four: 
model.pred4<-regsubsets(damt~., data.train.std.y)
summary(model.pred4)
low.bic.pred4<-which.min(model.pred4$bic)
low.bic.pred4
```


## Prediction model five: Best subset selection, k-fold cross validation (k=5), regressor = number of children

```{r}

cv.error.pred5=rep(0,5)
for (i in 1:5) {
  glm.model.pred5<-glm(damt ~ poly(chld, i), data= data.train.std.y)
  cv.error.pred5[i]=cv.glm(data.train.std.y,glm.model.pred5 ,K=5)$delta [1]
}
cv.error.pred5
## THe CV error values show that the first degree polynomial is the best fit on the regressor 'chld'. The CV error increases with polynomial degree.
model.pred5<-glm(damt~ chld, data = data.train.std.y)
pred.valid.pred5 <- predict(model.pred5, newdata = data.valid.std.y) # validation predictions
mpe.pred5<-mean((y.valid - pred.valid.pred5)^2) 
mpe.pred5
## 3.995
std.err.pred5<-sd((y.valid - pred.valid.pred5)^2)/sqrt(n.valid.y) 
std.err.pred5
## 0.27

##With a different Regressor - INCM, the median family income in a potential donor's neighborhood (in thousand $). 

cv.error.pred5.2=rep(0,5)
for (i in 1:5) {
  glm.model.pred5.2<-glm(damt ~ poly(incm, i), data= data.train.std.y)
  cv.error.pred5.2[i]=cv.glm(data.train.std.y,glm.model.pred5.2 ,K=5)$delta [1]
}
cv.error.pred5.2
## THe CV error values show that the first degree polynomial is the best fit for the regressor 'incm', which is the median family income in a potential donor's neighborhood.
model.pred5.2<-glm(damt~ incm, data = data.train.std.y)
pred.valid.pred5.2 <- predict(model.pred5.2, newdata = data.valid.std.y) # validation predictions
mpe.pred5.2<-mean((y.valid - pred.valid.pred5.2)^2) 
mpe.pred5.2
##4.32
std.err.pred5.2<-sd((y.valid - pred.valid.pred5.2)^2)/sqrt(n.valid.y) 
std.err.pred5.2
##.29

```

##Prediction model 6: Principal Components Regression:

```{r}
##Prediction model 6: Principal Components Regression:
model.pred6<-pcr(damt~., data=data.train.std.y ,scale=TRUE ,validation ="CV")
summary (model.pred6)
plot.pred6<-validationplot(model.pred6 ,val.type="MSEP", main= "Donation Amount: Cross Validation MSE for each number of components")
## The model with 20 components has the lowest CV score, as shown by both the summary and the graph. 
## Since this includes all 20 components, it is essentially the same as the least squares model. The mpe and std error calculated below are the same as the least squares model. 

pred.valid.pred6=predict (model.pred6 , newdata = data.valid.std.y, ncomp =20)
mpe.pred6<-mean((y.valid - pred.valid.pred6)^2)
mpe.pred6
## 1.8675
std.err.pred6<-sd((y.valid - pred.valid.pred6)^2)/sqrt(n.valid.y) 
std.err.pred6
## .16966
```

## Prediction Model 7: Partial Least Squares

```{r}
## Prediction Model 7: Partial Least Squares
model.pred7<-plsr(damt~., data=data.train.std.y ,scale=TRUE ,validation ="CV")
summary(model.pred7)
validationplot(model.pred7 ,val.type="MSEP", main = "Donation Amount: CV MSE for components, Partial Least Squares Model")
## % var explained is at it's maximum for 'damt' at m = 8 and above
## CV is at it's minimum at  m = 5 and above
## Use M = 5
pred.valid.pred7=predict (model.pred7 , newdata = data.valid.std.y, ncomp =5)
mpe.pred7<-mean((y.valid - pred.valid.pred7)^2)
mpe.pred7
## 1.866814
std.err.pred7<-sd((y.valid - pred.valid.pred7)^2)/sqrt(n.valid.y) 
std.err.pred7
## 0.1696325
```

## Prediction Model 8: Ridge Regression

```{r}

x=model.matrix(damt~., data.train.std.y)[,-1]
y=data.train.std.y$damt
lambdas <- 10^seq(10, -2, length = 100)
lambda.model.pred8<- cv.glmnet(x, y, alpha = 0, lambda = lambdas)
plot(lambda.model.pred8)
```


## Final Results

### Save test set classifications and predictions in cvs file. Name file with initals

```{r}

# Save final results for both classification and regression

length(chat.test)
length(yhat.test)
chat.test[1:10] 
yhat.test[1:10] 

ip <- data.frame(chat=chat.test, yhat=yhat.test)
write.csv(ip, file="ABC.csv", row.names=FALSE) 
```



```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

